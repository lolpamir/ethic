
import streamlit as st
import requests
from bs4 import BeautifulSoup
import google.generativeai as genai
from datetime import datetime
import random
import csv
import re
import os
import xml.etree.ElementTree as ET
from io import StringIO

# Gemini API ì„¤ì •
try:
    genai.configure(api_key=os.getenv("AIzaSyCjaXGNzHNhXDa1hmBnj0A6CyOgRG5q1vk"))
except Exception as e:
    st.error(f"Gemini API í‚¤ ì„¤ì • ì˜¤ë¥˜: {e}. Streamlit Cloudì˜ Secrets ì„¤ì •ì—ì„œ GEMINI_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

st.set_page_config(page_title="ì¸ê³µì§€ëŠ¥ê³¼ ìœ¤ë¦¬", layout="wide")
st.title("ğŸ“ ìµœê·¼ ê¸°ì‚¬ë¡œ ì•Œì•„ë³´ëŠ” AIì˜ ê¶Œë¦¬ì¹¨í•´")

# ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
DATA_DIR = "data"
if not os.path.exists(DATA_DIR):
    try:
        os.makedirs(DATA_DIR)
    except Exception as e:
        st.error(f"ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
        st.stop()

# ìš”ì•½ í•¨ìˆ˜ ì •ì˜
def summarize_article(article_text):
    try:
        model = genai.GenerativeModel("models/gemini-1.5-flash")
        prompt = (
            "ë‹¤ìŒ ê¸°ì‚¬ ë‚´ìš©ì„ ì„œë¡ , ë³¸ë¡ , ê²°ë¡ ì˜ êµ¬ì¡°ë¡œ ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì¤˜. "
            "ê° ë¶€ë¶„ì€ ì†Œì œëª©ìœ¼ë¡œ êµ¬ë¶„í•´ì¤˜.\n\n"
            f"{article_text}"
        )
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        return f"Gemini ìš”ì•½ ì‹¤íŒ¨: {e}"

# ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜ (newspaper ì œê±° ë²„ì „)
def fetch_article_text(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")
        paragraphs = soup.find_all("p")
        text = "\n".join([p.get_text(strip=True) for p in paragraphs])
        return text if len(text.strip()) > 200 else ""
    except Exception as e:
        st.error(f"ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""

# êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ ê¸°ì‚¬ ë§í¬ ê°€ì ¸ì˜¤ê¸° (50ê°œ ì¤‘ ëœë¤ 3ê°œ)
def get_google_news_links(keyword):
    try:
        query = f"AI+{keyword}"
        rss_url = f"https://news.google.com/rss/search?q={query}&hl=ko&gl=KR&ceid=KR:ko"
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(rss_url, headers=headers, timeout=10)
        response.raise_for_status()

        rss_content = StringIO(response.text)
        tree = ET.parse(rss_content)
        root = tree.getroot()

        all_links = []
        for item in root.findall(".//item")[:50]:
            title = item.find("title").text if item.find("title") is not None else ""
            link = item.find("link").text if item.find("link") is not None else ""
            if title and link:
                all_links.append((title, link))

        return random.sample(all_links, k=min(3, len(all_links)))
    except Exception as e:
        st.error(f"êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

# ê¶Œë¦¬ í‚¤ì›Œë“œ ì¶”ì • í•¨ìˆ˜
def infer_rights(text):
    keywords = {
        "ë…¸ë™": "ë…¸ë™ê¶Œ",
        "íŒŒì—…": "ë…¸ë™ê¶Œ",
        "ì €ì‘ê¶Œ": "ì €ì‘ê¶Œ",
        "í‘œì ˆ": "ì €ì‘ê¶Œ",
        "ì°¨ë³„": "í‰ë“±ê¶Œ",
        "ì„±ë³„": "í‰ë“±ê¶Œ",
        "ì‚¬ìƒí™œ": "í”„ë¼ì´ë²„ì‹œê¶Œ",
        "ê°ì‹œ": "í”„ë¼ì´ë²„ì‹œê¶Œ",
        "ì–¼êµ´ì¸ì‹": "í”„ë¼ì´ë²„ì‹œê¶Œ",
        "ê°œì¸ì •ë³´": "ê°œì¸ì •ë³´ë³´í˜¸ê¶Œ",
        "ìœ ì¶œ": "ê°œì¸ì •ë³´ë³´í˜¸ê¶Œ"
    }
    matched = set()
    for k, v in keywords.items():
        if k in text:
            matched.add(v)
    return list(matched)

# ê¶Œë¦¬ë³„ ê¸°ì‚¬ ê¸°ë¡ ì €ì¥
def save_to_csv(title, link, rights):
    csv_path = os.path.join(DATA_DIR, "data.csv")
    try:
        with open(csv_path, "a", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            for right in rights:
                writer.writerow([datetime.now().strftime("%Y-%m-%d %H:%M:%S"), title, link, right])
    except Exception as e:
        st.error(f"CSV ì €ì¥ ì˜¤ë¥˜: {e}")

# í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŒ… í•¨ìˆ˜
def highlight_keywords(text):
    for keyword in ["ë…¸ë™", "íŒŒì—…", "ì €ì‘ê¶Œ", "í‘œì ˆ", "ì°¨ë³„", "ì„±ë³„", "ì‚¬ìƒí™œ", "ê°ì‹œ", "ì–¼êµ´ì¸ì‹", "ê°œì¸ì •ë³´", "ìœ ì¶œ"]:
        text = re.sub(f"({keyword})", r"**\1**", text, flags=re.IGNORECASE)
    return text

col_left, col_center, col_right = st.columns([1, 4, 1])

# ì™¼ìª½ col_left: ë¡œê·¸ì¸ ë° ê°œë… ë³´ê¸° ë©”ë‰´
with col_left:
    st.subheader("ğŸ‘¥ ë¡œê·¸ì¸")
    login_role = st.radio("ê³„ì • ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”:", ["í•™ìƒ", "êµì‚¬"])
    st.session_state["login_role"] = login_role

    st.subheader("ğŸ“š ê¶Œë¦¬ ê°œë… ë³´ê¸°")
    selected_right = st.selectbox("ê´€ì‹¬ ìˆëŠ” ê¶Œë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:", ["ë…¸ë™ê¶Œ", "ì €ì‘ê¶Œ", "í‰ë“±ê¶Œ", "í”„ë¼ì´ë²„ì‹œê¶Œ", "ê°œì¸ì •ë³´ë³´í˜¸ê¶Œ"])
    rights_info = {
        "ë…¸ë™ê¶Œ": "ë…¸ë™ê¶Œì€ ëª¨ë“  ì‚¬ëŒì´ ì¸ê°„ë‹¤ìš´ ì‚¶ì„ ìœ„í•´ ì¼í•  ê¶Œë¦¬ì…ë‹ˆë‹¤. AIê°€ ì‚¬ëŒì„ ëŒ€ì²´í•˜ê±°ë‚˜ AIë¡œ ì¸í•œ ê°ì‹œÂ·í‰ê°€ê°€ ë…¸ë™í™˜ê²½ì„ ì•…í™”ì‹œí‚¤ëŠ” ê²½ìš° ì¹¨í•´ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nğŸ“œ ê´€ë ¨ ë²•ë ¹: ëŒ€í•œë¯¼êµ­ í—Œë²• ì œ32ì¡°, ê·¼ë¡œê¸°ì¤€ë²• ì œ1ì¡°",
        "ì €ì‘ê¶Œ": "ì €ì‘ê¶Œì€ ì°½ì‘ìê°€ ìì‹ ì˜ ì°½ì‘ë¬¼ì— ëŒ€í•´ ê°€ì§€ëŠ” ê¶Œë¦¬ì…ë‹ˆë‹¤. ìƒì„±í˜• AIê°€ ë¬´ë‹¨ìœ¼ë¡œ ì°½ì‘ë¬¼ì„ í•™ìŠµÂ·ìƒì„±í•˜ëŠ” ê²½ìš° ì¹¨í•´ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nğŸ“œ ê´€ë ¨ ë²•ë ¹: ì €ì‘ê¶Œë²• ì œ4ì¡°, ì œ97ì¡°ì˜5",
        "í‰ë“±ê¶Œ": "í‰ë“±ê¶Œì€ ì¸ì¢…, ì„±ë³„, ë‚˜ì´, ì¥ì•  ë“±ì— ê´€ê³„ì—†ì´ ë™ë“±í•˜ê²Œ ëŒ€ìš°ë°›ì„ ê¶Œë¦¬ì…ë‹ˆë‹¤. AI ì•Œê³ ë¦¬ì¦˜ì´ í¸í–¥ë˜ì—ˆì„ ê²½ìš° ì°¨ë³„ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nğŸ“œ ê´€ë ¨ ë²•ë ¹: ëŒ€í•œë¯¼êµ­ í—Œë²• ì œ11ì¡°",
        "í”„ë¼ì´ë²„ì‹œê¶Œ": "í”„ë¼ì´ë²„ì‹œê¶Œì€ ì‚¬ìƒí™œì˜ ììœ ì™€ ì¸ê²©ê¶Œì„ ë³´í˜¸ë°›ì„ ê¶Œë¦¬ì…ë‹ˆë‹¤. ì–¼êµ´ì¸ì‹ ê¸°ìˆ ì´ë‚˜ ê°ì‹œ ì‹œìŠ¤í…œì´ ê³¼ë„í•˜ê²Œ í™œìš©ë  ê²½ìš° ì¹¨í•´ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nğŸ“œ ê´€ë ¨ ë²•ë ¹: ê°œì¸ì •ë³´ ë³´í˜¸ë²• ì œ4ì¡°, í—Œë²• ì œ17ì¡°",
        "ê°œì¸ì •ë³´ë³´í˜¸ê¶Œ": "ê°œì¸ì •ë³´ë³´í˜¸ê¶Œì€ ì´ë¦„, ìƒë…„ì›”ì¼, ìœ„ì¹˜ ì •ë³´, ê±´ê°• ì •ë³´ ë“± ê°œì¸ ì •ë³´ë¥¼ ìˆ˜ì§‘Â·ì²˜ë¦¬Â·ìœ ì¶œë¡œë¶€í„° ë³´í˜¸ë°›ì„ ê¶Œë¦¬ì…ë‹ˆë‹¤. AI ì‹œìŠ¤í…œì´ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ ìœ ì¶œí•  ê²½ìš° ë¬¸ì œê°€ ë©ë‹ˆë‹¤.\n\nğŸ“œ ê´€ë ¨ ë²•ë ¹: ê°œì¸ì •ë³´ ë³´í˜¸ë²• ì œ15ì¡°~ì œ17ì¡°, ì •ë³´í†µì‹ ë§ë²•"
    }
    st.info(rights_info[selected_right])

# ê°€ìš´ë° col_center: ê¸°ì‚¬ ì¶œë ¥, ìš”ì•½, ì œì¶œ ê¸°ëŠ¥
with col_center:
    if "news_links" not in st.session_state:
        st.session_state["news_links"] = []

    st.subheader("ğŸ“Œ ê¸°ì‚¬ í‚¤ì›Œë“œ ì…ë ¥")
    keyword_input = st.text_input("ê¸°ì‚¬ ê²€ìƒ‰ í‚¤ì›Œë“œ (ì˜ˆ: ë…¸ë™ê¶Œ, ì €ì‘ê¶Œ ë“±)", key="keyword_input")

    if st.button("ğŸ“° AI ê´€ë ¨ê¸°ì‚¬ ì¶œë ¥"):
        if keyword_input.strip():
            st.session_state["news_links"] = get_google_news_links(keyword_input.strip())
        else:
            st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    if st.session_state["news_links"]:
        st.markdown("### ğŸ“Œ ìµœê·¼ AI ë…¼ë€ ê´€ë ¨ ê¸°ì‚¬ (ì¶œì²˜: Google News)")
        for idx, (title, link) in enumerate(st.session_state["news_links"], 1):
            text = fetch_article_text(link)
            inferred_rights = infer_rights(text)
            highlighted_title = highlight_keywords(title)

            st.markdown(f"**ê¸°ì‚¬ {idx}:** {highlighted_title}")
            st.markdown(f"ğŸ”— URL: {link}")

            if inferred_rights:
                st.markdown(f"ğŸ” ì¶”ì • ê¶Œë¦¬ ì¹¨í•´: {', '.join(set(inferred_rights))}")
                save_to_csv(title, link, inferred_rights)
            else:
                st.markdown("ğŸ” ê´€ë ¨ ê¶Œë¦¬ íƒìƒ‰ í•„ìš”")

    st.subheader("ğŸ“Œ ê¸°ì‚¬ ë§í¬ ì…ë ¥")
    url_input = st.text_input("ê¸°ì‚¬ ë§í¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", key="url_input")

    if st.button("ğŸ§  ìš”ì•½í•˜ê¸°", key="summarize_button"):
        if not url_input.strip():
            st.warning("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            st.markdown(f"### ğŸ”— ê¸°ì‚¬ ë§í¬: [{url_input}]({url_input})")
            text = fetch_article_text(url_input)
            if text:
                with st.spinner("ìš”ì•½ ì¤‘ì…ë‹ˆë‹¤..."):
                    summary = summarize_article(text)
                st.markdown(f"#### ğŸ“‹ ìš”ì•½ ê²°ê³¼")
                st.write(summary)
            else:
                st.error("âŒ ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë§í¬ê°€ ìœ íš¨í•œ ë‰´ìŠ¤ ê¸°ì‚¬ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

    st.markdown("#### âœï¸ ê¸°ì‚¬ì— ëŒ€í•œ AIì™€ ì¸ê°„ì˜ ê¶Œë¦¬ì— ëŒ€í•œ ìƒê°ì„ ì ì–´ë³´ì„¸ìš”.")
    user_name = st.text_input("ì´ë¦„(íŒ€ëª…)ì„ ì…ë ¥í•˜ì„¸ìš”", key="name_input")
    user_thought = st.text_area("ìƒê°ì„ ì‘ì„±í•˜ì„¸ìš”", key="thought_input")

    if st.button("ì œì¶œí•˜ê¸°", key="submit_thought"):
        if user_name.strip() == "" or user_thought.strip() == "":
            st.warning("âš ï¸ ì´ë¦„ê³¼ ìƒê°ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            txt_path = os.path.join(DATA_DIR, "data.txt")
            try:
                with open(txt_path, "a", encoding="utf-8") as f:
                    f.write( f,"[{timestamp}] {user_name}:\n{user_thought}\n\n")
                st.success("âœ… ìƒê°ì´ ì„±ê³µì ìœ¼ë¡œ ì œì¶œë˜ì—ˆìŠµë‹ˆë‹¤!")
            except Exception as e:
                st.error(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

    if st.session_state.get("login_role") == "êµì‚¬":
        st.markdown("---")
        st.subheader("ğŸ“‹ í•™ìƒ ì œì¶œ ë‚´ìš© ì—´ëŒ")
        txt_path = os.path.join(DATA_DIR, "data.txt")
        if os.path.exists(txt_path):
            with open(txt_path, "r", encoding="utf-8") as f:
                st.text(f.read())
        else:
            st.info("ì•„ì§ ì œì¶œëœ í•™ìƒì˜ ìƒê°ì´ ì—†ìŠµë‹ˆë‹¤.")

# ì˜¤ë¥¸ìª½ col_right: ì›ë˜ì˜ Tips
with col_right:
    st.subheader("Tips...")
    st.info("""
    ğŸ‘€ ì´ ì•±ì€ ìµœê·¼ ê¸°ì‚¬ë¡œ ì¹¨í•´ë˜ëŠ” ë‹¤ì–‘í•œ ê¶Œë¦¬ë¥¼ íƒìƒ‰í•˜ê¸° ìœ„í•œ ì•±ì…ë‹ˆë‹¤
    ğŸ“Œ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•˜ê³  urlì„ ë³µì‚¬, ë¶™ì—¬ë„£ê¸°ë¡œ ìš”ì•½í•˜ê³  ì°¸ê³ í•˜ì„¸ìš”
    ğŸ§­ AIì— ì˜í•´ ì¹¨í•´ë˜ëŠ” ê¶Œë¦¬ë¥¼ ë” ì°¾ì•„ì£¼ì„¸ìš”
    """)
